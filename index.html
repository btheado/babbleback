<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Babbleback</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
            text-align: center;
        }

        h1 {
            color: #333;
        }

        .controls {
            margin: 20px 0;
        }

        button {
            background-color: #4CAF50;
            border: none;
            color: white;
            padding: 10px 20px;
            text-align: center;
            text-decoration: none;
            display: inline-block;
            font-size: 16px;
            margin: 10px;
            cursor: pointer;
            border-radius: 4px;
        }

        button:disabled {
            background-color: #cccccc;
            cursor: not-allowed;
        }

        #stopButton {
            background-color: #f44336;
        }

        #status {
            margin-top: 10px;
            padding: 10px;
            border-radius: 4px;
            background-color: #f1f1f1;
        }

        .waveform-container {
            position: relative;
            width: 100%;
            height: 100px;
            margin: 20px 0;
            background-color: #f9f9f9;
            border: 1px solid #ddd;
            border-radius: 4px;
            overflow: hidden;
        }

        #waveformCanvas {
            width: 100%;
            height: 100%;
            position: absolute;
            top: 0;
            left: 0;
        }

        .marker {
            position: absolute;
            top: 0;
            width: 2px;
            height: 100%;
            background-color: rgba(255, 0, 0, 0.7);
        }

        .playback-marker {
            left: 0;
        }

        .record-marker {
            right: 0;
        }

        .time-indicator {
            display: flex;
            justify-content: space-between;
            width: 100%;
            padding: 5px 0;
        }

        .delay-control {
            margin: 20px 0;
            padding: 15px;
            background-color: #f1f1f1;
            border-radius: 4px;
        }

        .slider-container {
            display: flex;
            align-items: center;
            justify-content: center;
            gap: 15px;
        }

        #delaySlider {
            width: 300px;
        }

        #delayValue {
            font-weight: bold;
            min-width: 80px;
        }

        .video-container {
            display: flex;
            flex-direction: row;
            justify-content: space-between;
            width: 100%;
            margin: 20px 0;
        }

        .video-wrapper {
            width: 48%;
            position: relative;
        }

        .video-wrapper h3 {
            margin-top: 0;
        }

        #recordVideo, #playbackVideo {
            width: 100%;
            background-color: #000;
            border-radius: 4px;
            object-fit: contain; /* Maintain aspect ratio */
        }

        #recordCanvas, #playbackCanvas {
            display: none;
        }

        .video-label {
            position: absolute;
            top: 10px;
            left: 10px;
            background-color: rgba(0, 0, 0, 0.5);
            color: white;
            padding: 5px 10px;
            border-radius: 4px;
            font-size: 14px;
        }
    </style>
</head>
<body>
    <h1>Babbleback</h1>
    <p>This application records audio and video from your devices and plays them back with a configurable delay.</p>

    <div class="delay-control">
        <div class="slider-container">
            <span>Delay:</span>
            <input type="range" id="delaySlider" min="3" max="20" value="5" step="0.5">
            <span id="delayValue">5.0 seconds</span>
        </div>
    </div>

    <div class="controls">
        <button id="startButton">Start</button>
        <button id="stopButton">Stop</button>
    </div>

    <div id="status">Ready to start recording.</div>

    <div class="video-container">
        <div class="video-wrapper">
            <h3>Delayed Playback</h3>
            <video id="playbackVideo" autoplay playsinline></video>
            <canvas id="playbackCanvas"></canvas>
            <div class="video-label" id="playbackLabel">5s Delay</div>
        </div>
        <div class="video-wrapper">
            <h3>Live Input</h3>
            <video id="recordVideo" autoplay muted playsinline></video>
            <canvas id="recordCanvas"></canvas>
            <div class="video-label">Live</div>
        </div>
    </div>

    <div class="waveform-container">
        <canvas id="waveformCanvas"></canvas>
        <div class="marker playback-marker"></div>
        <div class="marker record-marker"></div>
    </div>

    <div class="time-indicator">
        <span>Playback (now)</span>
        <span id="delayIndicator">5-second delay</span>
        <span>Recording (now)</span>
    </div>

    <script>
        // Get references to the UI elements
        const startButton = document.getElementById('startButton');
        const stopButton = document.getElementById('stopButton');
        const statusDisplay = document.getElementById('status');
        const waveformCanvas = document.getElementById('waveformCanvas');
        const canvasCtx = waveformCanvas.getContext('2d');
        const delaySlider = document.getElementById('delaySlider');
        const delayValue = document.getElementById('delayValue');
        const delayIndicator = document.getElementById('delayIndicator');
        const playbackLabel = document.getElementById('playbackLabel');

        // Video elements
        const recordVideo = document.getElementById('recordVideo');
        const playbackVideo = document.getElementById('playbackVideo');
        const recordCanvas = document.getElementById('recordCanvas');
        const playbackCanvas = document.getElementById('playbackCanvas');
        const tempCanvas = document.createElement('canvas');
        const recordCtx = recordCanvas.getContext('2d');
        const playbackCtx = playbackCanvas.getContext('2d');
        const tempCtx = tempCanvas.getContext('2d');

        // Audio context and nodes
        let audioContext;
        let mediaStream;
        let sourceNode;
        let delayNode;
        let analyserNode;
        let recording = false;
        let animationId;

        // Current delay value
        let currentDelay = parseFloat(delaySlider.value);

        // Maximum buffer size in seconds
        const maxBufferSize = 20; // Match the maximum slider value

        // Frame buffer for video
        const frameRate = 30; // Frames per second for video
        const frameDelay = 1000 / frameRate; // Milliseconds between frames
        const frameBufferSize = maxBufferSize * frameRate;
        let frameBuffer = [];
        let frameBufferPosition = 0;

        // Video dimensions
        let actualVideoWidth = 0;
        let actualVideoHeight = 0;

        // Buffer for storing audio data
        const sampleRate = 60; // Samples per second for visualization
        const totalSamples = maxBufferSize * sampleRate;
        let audioBuffer = new Array(totalSamples).fill(0);
        let bufferPosition = 0; // Current position in the buffer

        // Update delay value display
        delaySlider.addEventListener('input', function() {
            const newDelay = parseFloat(this.value);
            delayValue.textContent = newDelay.toFixed(1) + ' seconds';
            delayIndicator.textContent = newDelay.toFixed(1) + '-second delay';
            playbackLabel.textContent = newDelay.toFixed(1) + 's Delay';

            if (recording && delayNode) {
                // Update the delay node's delay time
                delayNode.delayTime.setValueAtTime(newDelay, audioContext.currentTime);
                currentDelay = newDelay;
            } else {
                currentDelay = newDelay;
            }
        });

        // Set canvas dimensions
        function resizeCanvas() {
            const dpr = window.devicePixelRatio || 1;

            // Waveform canvas
            const waveRect = waveformCanvas.getBoundingClientRect();
            waveformCanvas.width = waveRect.width * dpr;
            waveformCanvas.height = waveRect.height * dpr;
            canvasCtx.scale(dpr, dpr);
            waveformCanvas.style.width = waveRect.width + 'px';
            waveformCanvas.style.height = waveRect.height + 'px';
        }

        window.addEventListener('resize', resizeCanvas);
        resizeCanvas();

        // Initialize the application
        startButton.addEventListener('click', startDelayedPlayback);
        stopButton.addEventListener('click', stopDelayedPlayback);
        stopButton.disabled = true;

        // Function to start recording and playback
        async function startDelayedPlayback() {
            try {
                // Create a new audio context if one doesn't exist
                if (!audioContext) {
                    audioContext = new (window.AudioContext || window.webkitAudioContext)();
                }

                // Get the current delay value
                currentDelay = parseFloat(delaySlider.value);

                // Request microphone and camera access with appropriate constraints
                mediaStream = await navigator.mediaDevices.getUserMedia({
                    // Claude added these as part of my request for it to fix audio/video sync,
                    // but I removed it, thinking that disabling these might introduce audio
                    // feedback loops. The sync issue seems to be fixed without it.
                    /*
                    audio: {
                        echoCancellation: false,  // Disable echo cancellation for better audio quality
                        noiseSuppression: false,  // Disable noise suppression
                        autoGainControl: false    // Disable auto gain
                    },
                    */
                    audio: true,
                    video: {
                        width: { ideal: 640 },
                        height: { ideal: 480 },
                        frameRate: { ideal: frameRate } // Request consistent frame rate
                    }
                });

                // Set the record video source
                recordVideo.srcObject = mediaStream;

                // Reset audio sync tracking variables
                audioStartTime = 0;

                // Wait for metadata to load to get actual video dimensions
                recordVideo.onloadedmetadata = function() {
                    actualVideoWidth = recordVideo.videoWidth;
                    actualVideoHeight = recordVideo.videoHeight;

                    // Update canvas dimensions to match actual video size
                    recordCanvas.width = actualVideoWidth;
                    recordCanvas.height = actualVideoHeight;
                    playbackCanvas.width = actualVideoWidth;
                    playbackCanvas.height = actualVideoHeight;
                    tempCanvas.width = actualVideoWidth;
                    tempCanvas.height = actualVideoHeight;

                    statusDisplay.textContent = `Recording with ${actualVideoWidth}x${actualVideoHeight} resolution`;
                };

                // Create the source node from the microphone stream
                sourceNode = audioContext.createMediaStreamSource(mediaStream);

                // Create a delay node with the maximum possible delay time
                delayNode = audioContext.createDelay(maxBufferSize);
                delayNode.delayTime.value = currentDelay;

                // Create an analyser node for visualizing the audio
                analyserNode = audioContext.createAnalyser();
                analyserNode.fftSize = 2048;

                // Connect the nodes: source -> analyser -> delay -> destination (speakers)
                sourceNode.connect(analyserNode);
                analyserNode.connect(delayNode);
                delayNode.connect(audioContext.destination);

                // Reset the buffers
                audioBuffer = new Array(totalSamples).fill(0);
                bufferPosition = 0;
                frameBuffer = new Array(frameBufferSize);
                frameTimestamps = new Array(frameBufferSize);
                frameBufferPosition = 0;

                // Start visualization
                visualize();

                // Start video frame capture and playback
                recording = true;
                captureAndPlayFrames();

                // Update UI
                startButton.disabled = true;
                stopButton.disabled = false;
                statusDisplay.textContent = `Recording and playing with ${currentDelay.toFixed(1)}-second delay...`;

            } catch (error) {
                console.error('Error accessing media devices:', error);
                statusDisplay.textContent = `Error: ${error.message}`;
            }
        }

        // Function to stop recording and playback
        function stopDelayedPlayback() {
            if (recording) {
                // Stop visualization
                cancelAnimationFrame(animationId);

                // Disconnect nodes
                if (sourceNode) {
                    sourceNode.disconnect();
                }
                if (analyserNode) {
                    analyserNode.disconnect();
                }
                if (delayNode) {
                    delayNode.disconnect();
                }

                // Stop all tracks from the media stream
                if (mediaStream) {
                    mediaStream.getTracks().forEach(track => track.stop());
                }

                // Clear video sources
                recordVideo.srcObject = null;
                playbackVideo.srcObject = null;

                // Update UI
                recording = false;
                startButton.disabled = false;
                stopButton.disabled = true;
                statusDisplay.textContent = 'Recording stopped.';

                // Clear the canvas
                const width = waveformCanvas.width / window.devicePixelRatio;
                const height = waveformCanvas.height / window.devicePixelRatio;
                canvasCtx.clearRect(0, 0, width, height);

                // Clear video canvases
                recordCtx.clearRect(0, 0, recordCanvas.width, recordCanvas.height);
                playbackCtx.clearRect(0, 0, playbackCanvas.width, playbackCanvas.height);
            }
        }

        // Store frame timestamps for synchronization
        let frameTimestamps = new Array(frameBufferSize);
        let audioStartTime = 0;

        // Function to capture and play video frames with delay
        function captureAndPlayFrames() {
            if (!recording) return;

            // Only proceed if we have valid video dimensions
            if (actualVideoWidth === 0 || actualVideoHeight === 0) {
                setTimeout(captureAndPlayFrames, frameDelay);
                return;
            }

            // Get current time - we'll use this for synchronization
            const currentTime = audioContext.currentTime;

            // Set audio start time reference if this is the first frame
            if (audioStartTime === 0) {
                audioStartTime = currentTime;
            }

            // Capture current frame
            recordCtx.drawImage(recordVideo, 0, 0, recordCanvas.width, recordCanvas.height);

            // Store the frame in the buffer along with its timestamp
            frameBuffer[frameBufferPosition] = recordCtx.getImageData(0, 0, recordCanvas.width, recordCanvas.height);
            frameTimestamps[frameBufferPosition] = currentTime;

            // Calculate playback time (based on audio context time)
            const playbackTime = currentTime - currentDelay;

            // Find the closest frame that matches our desired playback time
            let closestFramePos = -1;
            let minTimeDiff = Infinity;

            // Search for the frame with timestamp closest to our target playback time
            for (let i = 0; i < frameBufferSize; i++) {
                if (frameTimestamps[i] && frameBuffer[i]) {
                    const timeDiff = Math.abs(frameTimestamps[i] - playbackTime);
                    if (timeDiff < minTimeDiff) {
                        minTimeDiff = timeDiff;
                        closestFramePos = i;
                    }
                }
            }

            // If we found a suitable frame, display it
            if (closestFramePos !== -1 && frameBuffer[closestFramePos]) {
                // Draw the stored image data to the temp canvas
                tempCtx.putImageData(frameBuffer[closestFramePos], 0, 0);

                // Draw from temp canvas to playback canvas
                playbackCtx.drawImage(tempCanvas, 0, 0);

                // Create a stream from the canvas for the playback video
                if (!playbackVideo.srcObject) {
                    const playbackStream = playbackCanvas.captureStream(frameRate);
                    playbackVideo.srcObject = playbackStream;
                }
            }

            // Update buffer position
            frameBufferPosition = (frameBufferPosition + 1) % frameBufferSize;

            // Schedule the next frame capture
            setTimeout(captureAndPlayFrames, frameDelay);
        }

        // Function to visualize the audio
        function visualize() {
            // Get canvas dimensions
            const width = waveformCanvas.width / window.devicePixelRatio;
            const height = waveformCanvas.height / window.devicePixelRatio;

            // Create a buffer for the analyzer data
            const bufferLength = analyserNode.frequencyBinCount;
            const dataArray = new Uint8Array(bufferLength);

            // Clear the canvas
            canvasCtx.clearRect(0, 0, width, height);

            // Last update time for throttling updates
            let lastUpdateTime = 0;

            // Function to draw the waveform
            function draw(timestamp) {
                animationId = requestAnimationFrame(draw);

                // Throttle updates to every ~16ms (roughly 60fps)
                if (timestamp - lastUpdateTime < 16) {
                    return;
                }
                lastUpdateTime = timestamp;

                // Get the current audio data
                analyserNode.getByteTimeDomainData(dataArray);

                // Calculate the current audio volume/amplitude (simplified)
                let sum = 0;
                for (let i = 0; i < bufferLength; i++) {
                    sum += Math.abs(dataArray[i] - 128);
                }
                const volume = sum / bufferLength;

                // Store the current volume in the circular buffer
                audioBuffer[bufferPosition] = volume;
                bufferPosition = (bufferPosition + 1) % totalSamples;

                // Clear the canvas
                canvasCtx.fillStyle = '#f9f9f9';
                canvasCtx.fillRect(0, 0, width, height);

                // Draw the grid lines
                canvasCtx.beginPath();
                canvasCtx.strokeStyle = '#ddd';
                canvasCtx.lineWidth = 1;

                // Vertical grid lines (time markers)
                // Draw lines based on current delay
                const gridLines = Math.min(10, Math.max(3, Math.floor(currentDelay)));
                for (let i = 0; i <= gridLines; i++) {
                    const x = (width / gridLines) * i;
                    canvasCtx.moveTo(x, 0);
                    canvasCtx.lineTo(x, height);
                }

                // Horizontal grid line (center)
                canvasCtx.moveTo(0, height / 2);
                canvasCtx.lineTo(width, height / 2);
                canvasCtx.stroke();

                // Draw the waveform
                canvasCtx.beginPath();
                canvasCtx.strokeStyle = '#2196F3';
                canvasCtx.lineWidth = 2;

                // Scale the waveform to fill the canvas
                const scale = height / 40;  // Adjusted for typical volume values

                // Calculate how many samples to display based on current delay
                const samplesToDisplay = currentDelay * sampleRate;

                for (let i = 0; i < samplesToDisplay; i++) {
                    // Calculate the position in the circular buffer
                    let bufferIndex = (bufferPosition - 1 - i + totalSamples) % totalSamples;

                    // Calculate x position (right to left)
                    const x = width - (i / samplesToDisplay) * width;

                    // Calculate y position (centered with amplitude)
                    const y = (height / 2) - (audioBuffer[bufferIndex] * scale);

                    if (i === 0) {
                        canvasCtx.moveTo(x, y);
                    } else {
                        canvasCtx.lineTo(x, y);
                    }
                }

                canvasCtx.stroke();
            }

            draw(0);
        }

        // Function to resume audio context (needed for browsers that suspend audio context until user interaction)
        document.addEventListener('click', function() {
            if (audioContext && audioContext.state === 'suspended') {
                audioContext.resume();
            }
        }, { once: true });
    </script>
</body>
</html>
